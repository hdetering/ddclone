{
    "contents" : "source('R/genotype.R')\nsource('R/env-setup.R')\n\nlibrary(VGAM)\nlibrary(vegan)\nlibrary(matrixStats)\n\nEmptyCache <- T\n\nmake.pyclone.input <- function(mutDat) {\n  # mutation_id  ref_counts  var_counts\tnormal_cn\tminor_cn\tmajor_cn\n  dat <- data.frame(t(mutDat$mutCounts), stringsAsFactors=F)\n  # reference counts should be total counts - variant counts (in mutCounts, first row is d, total number of reads)\n  colnames(dat) <- c('total_counts', 'var_counts')\n  dat$ref_counts <- dat$total_counts - dat$var_counts\n  stopifnot(dat$ref_counts >= 0)\n  nMut <- length(mutDat$mutPrevalence)\n  # mutationId should be read off the mutDat\n  dat$mutation_id <- colnames(mutDat$filteredMutMatrix)\n\n  for (i in seq(nMut)) {\n    dat$normal_cn[i] <- genotype.c(mutDat$psi[[i]]$gN)\n    # should we use min for minor?\n    b <- genotype.b(mutDat$psi[[i]]$gV)\n    a <- genotype.a(mutDat$psi[[i]]$gV)\n    dat$minor_cn[i] <- min(b, a)\n    dat$major_cn[i] <- max(b, a)\n  }\n\n  dat\n}\n\nrelabel.clusters <- function(numericVector) {\n  t <- as.factor(x = numericVector)\n  levels(t) <- 1:length(levels(t))\n  as.numeric(t)\n}\n\n# -- Clustering evaluation and prevalence accuracy\n# returns -sum(pk * log(pk), axis=0).\n# ref: http://pydoc.net/Python/scikit-learn/0.14.1/sklearn.metrics.cluster.supervised/\nentropy <- function(labels) {\n  if (length(labels) == 0)\n    return(1.0)\n  pi <- table(labels)\n  pi_sum <- sum(pi)\n  return(-sum((pi / pi_sum) * (log(pi) - log(pi_sum))))\n}\n\n# MI(U,V)=\\sum_{i=1}^R \\sum_{j=1}^C P(i,j)\\log\\\\frac{P(i,j)}{P(i)P'(j)}\nmutual.info.score <- function(trueLabels, predLabels) {\n  # contingency table\n  contingency <- ftable(trueLabels, predLabels)\n  contingency_sum <- sum(contingency)\n  pi <- rowSums(contingency) # row\n  pj <- colSums(contingency) # column\n  outer <-  pi %o% pj\n  nnz <- contingency != 0.0\n  # normalized contingency\n  contingency_nm <- contingency[nnz]\n  log_contingency_nm <- log(contingency_nm)\n  contingency_nm <- contingency_nm / contingency_sum\n  log_outer <- -log(outer[nnz]) + log(sum(pi)) + log(sum(pj))\n  mi <- (contingency_nm * (log_contingency_nm - log(contingency_sum))\n         + contingency_nm * log_outer)\n  sum(mi)\n}\n\n# V-measure for clustering\nevaluate.clustering <- function(labels_true, labels_pred) {\n  if (length(labels_true) == 0)\n    return(list(homogeneity=1.0, completeness=1.0, v_measure_score=1.0))\n\n  entropy_C <- entropy(labels_true)\n  entropy_K <- entropy(labels_pred)\n\n  MI <- mutual.info.score(labels_true, labels_pred)\n\n  homogeneity <- ifelse(entropy_C != 0, MI / (entropy_C), 1.0)\n  completeness <- ifelse(entropy_K != 0, MI / (entropy_K), 1.0)\n\n  if (homogeneity + completeness == 0.0)\n    v_measure_score <- 0.0\n  else\n    v_measure_score <- (2.0 * homogeneity * completeness/ (homogeneity + completeness))\n\n  list(homogeneity=homogeneity, completeness=completeness, v_measure_score=v_measure_score)\n}\n\nevaluate.prevalence <- function(truePhi, predPhi) {\n  mean(abs(truePhi - predPhi))\n}\n\n\n# Absolute difference of mean MCMC-samples and true value\nevaluate.prevalence.trace <- function(state, trace, trueState, burn.in) {\n  # trace: matrix[values , mutations]\n  trace <- trace[-c(1:burn.in), ]\n  means <- colMeans(trace)\n  abs(means - trueState)\n}\n\n\n# --- Cached Variables --- (helps avoid copying while passing functions)\nLCACHED <- NULL\nAlphaCACHED <- NULL\nDecay.CACHED <- NULL\n\n## cached methods\n\n# mS: grid size for precision\n# mPhi: grid size for monte carlo estiamte of prevalence\n# THIS IS THE NATURAL LOGARITHM OF THE FUNCTION\nmake.cached.genotype.aware.likelihood <- function(mPhi=1000, dat, mS=10, simulated.data.id, psi.priors, range, tumourContent) {\n  fileName <- paste0('l', mPhi, '-', mS, '-', simulated.data.id)\n  baseDir <- get.path('likelihoods')\n  filePath <- file.path(baseDir, fileName)\n  if (file.exists(filePath) && !EmptyCache) {\n    readRDS(filePath)\n  } else {\n    phi <- seq(0, 1, length=mPhi)\n    psi <- psi.priors\n\n    # make grid for s\n    s <- seq(range$min, range$max, length=mS)\n    # for each s, phi in rows, and F in columns over observations [F(phi, obs)] sum(prod(rows[obs]))\n    ncol <- nrow(dat)\n    dims <- c(mS, mPhi, ncol)\n\n    distMat <- array(0, dim = dims)\n\n    sArray <- array(rep(s, mS), dim=c(mS, mPhi))\n    for (i in 1:ncol) { # per mutation\n      bArray <- array(dat$var_counts[i], dim=c(mS, mPhi))\n      dArray <- array(dat$total_counts[i], dim=c(mS, mPhi))\n      for (psi_i in psi[[i]]) {\n        eArray <- array(rep(pyclone.eta(phi = phi, psi = psi_i, t = tumourContent), each=mS), dim=c(mS, mPhi))\n        distMat[, , i] <- distMat[, , i] + beta.binomial(bArray, dArray, eArray, sArray)\n      }\n    }\n\n    distMat <- safelog(distMat)\n\n    res <- list(mat=distMat, phi=phi, s=s)\n    saveRDS(res, filePath)\n    res\n  }\n}\n\n# returns logged likelihood\ncached.pyclone.dd.crp.likelihood <- function(dat) {\n  # for now return a monte carlo estimate\n  function(obs.indices, precision) {\n    discMat <- LCACHED$mat[which.min(abs(precision-LCACHED$s)), , ]\n\n    if (length(obs.indices) == 1) {\n      val <- discMat[, as.vector(obs.indices)]\n      if (any(val == -Inf)) return(-Inf)\n      logSumExp(val) - log(nrow(discMat))\n    } else {\n      val <- rowSums(discMat[, as.vector(obs.indices)])\n      if (any(val == -Inf)) return(-Inf)\n      logSumExp(val) - log(nrow(discMat)) # makes NaN\n    }\n  }\n}\n\n\n# should be done for current value of a, the decay function parameter\ncached.resample.alpha <- function(state, a) {\n  alpha <- AlphaCACHED$alpha\n  l.p.c.Grid <- AlphaCACHED$grid[[which.min(abs(a - Decay.CACHED$a))]]\n  K <- sum(state$idx == state$customer)\n  prob <- exp(K * safelog(alpha) + l.p.c.Grid)\n  sample(alpha, 1, prob=prob)\n}\n\n\n# TODO: Do I need to include the f(d_ij) in the cluster probabilities?\n# for now I'm assuming no, because f is not influenced by s\nresample.neal.precision <- function(state, cluster.fn, curPrecision) {\n  alphaS <- 1.0\n  betaS <- 0.0001\n  # use the full conditional S according to Neal's paper (prior X F)\n  s <- LCACHED$s\n  lprior <- dgamma(s, alphaS, betaS, log =T)\n  llhood <- rep(0, length(s))\n\n  for (sVal in s) {\n    tmpLHood <- 0\n    for (i in unique(state$cluster)) {\n      obs.indexes <- which(state$cluster == i)\n      tmpLHood <- tmpLHood + cluster.fn(obs.indexes, precision = sVal)\n    }\n    llhood[length(llhood + 1)] <- tmpLHood\n  }\n\n  lprob <- lprior + llhood\n  sample(x = s, size=1, replace = F, prob = exp(lprob))\n}\n\nresample.pyclone.precision <- function(state, cluster.fn, curPrecision) {\n  alphaS <- 1.0\n  betaS <- 0.0001\n  # use the full conditional S according to Neal's paper (prior X F)\n  oldS <- curPrecision\n  proposalS <- .01\n  newS <- sample.gamma.proposal(oldS, proposalS)\n  if (is.nan(newS) || is.na(newS)) return(oldS)\n\n  l.oldTarget <- dgamma(oldS, alphaS, betaS, log =T)\n  l.newTarget <- dgamma(newS, alphaS, betaS, log =T)\n\n  for (i in unique(state$cluster)) {\n    obs.indexes <- which(state$cluster == i)\n    # cluster.fn which is the likelihood.fn is already in log space\n    l.oldTarget <- l.oldTarget + cluster.fn(obs.indexes, precision = oldS)\n    l.newTarget <- l.newTarget + cluster.fn(obs.indexes, precision = newS)\n  }\n\n  # x -> x'\n  params <- pyclone.gamma.params(oldS, proposalS)\n  l.forwardProposal <- dgamma(newS, params$a, params$b, log = T)\n\n  # x' -> x\n  params <- pyclone.gamma.params(newS, proposalS)\n  l.reverseProposal <- dgamma(oldS, params$a, params$b, log = T)\n\n  # use a simple MH ratio, just newJoint/oldJoint\n  MHRatio <- exp(l.newTarget-l.oldTarget)\n  if (is.nan(MHRatio) || is.na(MHRatio)) {\n    return(oldS)\n  } else {\n    return(ifelse(rbinom(1, 1, min(MHRatio, 1) ) == 1, newS, oldS))\n  }\n}\n\n# resamples a\ncached.resmple.decay.fn.param <- function(state, alpha) {\n  a <- Decay.CACHED$a\n\n  noSelfLoopIndices <- which(state$idx != state$customer)\n\n  theSum <- rep(0, length(a))\n  for (n_a in seq(length(a))) {\n    for (i in noSelfLoopIndices) {\n      theSum[n_a] <- theSum[n_a] + safelog(Decay.CACHED$decays[[n_a]][i, state$customer[i]])\n    }\n  }\n\n  N <- nrow(Decay.CACHED$decays)\n  for (n_a in seq(length(a))) {\n    for (i in seq(N)) {\n      temp <- ifelse(i > 1, safelog(alpha + sum(Decay.CACHED$decays[[n_a]][i, 1:(i-1)])), safelog(alpha))\n      theSum[n_a] <- theSum[n_a] - temp\n    }\n    # using an exponential prior\n    lambda <- 10\n    theSum[n_a] <- theSum[n_a] + log(lambda) - lambda * a[n_a]\n  }\n\n  # just added - log.sum(theSum), check if it's any better\n  sample(a, 1, replace = F, prob = exp(theSum - log.sum(theSum)))\n}\n\n\n# FOR EACH VALUE OF a, the decay function parameter\n# Dirichlet process concentration parameter\n# returns log.p(alpha) - sum(log(alpha + sum(f(dij))))\n# lacks the term for k * log(alpha)\nmake.cached.alpha <- function(M=1000, distMat, decay.fn, decay.fn.name, simulated.data.id, range) {\n  fileName <- paste0('alpha.',  M, '-', simulated.data.id)\n  baseDir <- get.path('likelihoods')\n  filePath <- file.path(baseDir, fileName)\n  if (file.exists(filePath) && !EmptyCache) {\n    readRDS(filePath)\n  } else {\n    # pick an equally distanced grid in (, 10), how about sampling from gamma?\n    alpha <- seq(range$min, range$max, length=M)\n    #cachedAlpha <- make.cached.decay(M, distMat, decay.fn, decay.fn.name)\n    l.p.c <- list()\n    # over different values of a (decay.param)\n    for (i in seq(length(Decay.CACHED$a))) {\n      # set f(i,i)=alpha, then there's one operation less at lesat\n      cachedDecay <- Decay.CACHED$decays[[i]] #f(d_ij)\n      totalDist <- rowSums(cachedDecay) - diag(cachedDecay) # vector of size N  (sum(f(dij)) over j!=i)\n      # should later add k.log.alpha\n      xt1 <- safelog(t(sapply(seq(alpha), function(j) {alpha[j] + totalDist})))\n      xt <- rowSums(xt1)\n      l.p.c[[i]] <- dgamma(alpha, shape=1, rate=0.4, log=T) - xt\n    }\n\n    res <- list(grid=l.p.c, alpha=alpha)\n    saveRDS(res, filePath)\n\n    res\n  }\n}\n\n\ninspect.alpha.prior <- function() {\n  y=dgamma(alpha, shape=1, rate=0.4, log=T)\n  plot(alpha, exp(y), xlim=c(0,20))\n  lines(alpha, y=rep(0, length(alpha)))\n}\n\nmake.cached.decay <- function(M=1000, distMat, decay.fn, decay.fn.name, simulated.data.id, range) {\n  fileName <- paste0('decay.fn.', decay.fn.name,  M, '-', simulated.data.id)\n  baseDir <- get.path('likelihoods')\n  filePath <- file.path(baseDir, fileName)\n  if (file.exists(filePath) && !EmptyCache) {\n    readRDS(filePath)\n  } else {\n    # make the grid\n    a <- seq(range$min, range$max, length=M)\n    decays <- lapply(a, FUN = function(a_i) { apply(distMat, c(1,2), function(d_ij) (decay.fn(d_ij, a_i)))} )\n    res <- list(a=a, decays=decays)\n    saveRDS(res, filePath)\n    res\n  }\n}\n\n\n# use http://cran.r-project.org/web/packages/Rmpfr/vignettes/Rmpfr-pkg.pdf\n# to fix underflow\n# given current state (table assignments) sample parameters for each cluster\n# # ref: Sudderth PhD Thesis: Alg2.1. step 3:  θ^t_k ∼ p(θ_k |{x_i | z^t_i = k},λ)  {page 88}\n# why isn't the prior being used? IT\"S all based on the likelihood -> because p(theta | lambda) is uniform (uniform prior)\ncached.sample.pyclone.cluster.parameters <- function(state, precision) {\n  state$phi <- seq(nrow(state))\n  probs <- list()\n  discMat <- LCACHED$mat[which.min(abs(precision-LCACHED$s)), , ]\n  for (clust in unique(state$cluster)) {\n    obs.indices <- which(state$cluster == clust)\n    if (length(obs.indices) == 1) {\n      t <- discMat[, obs.indices]\n      prob <- exp(t - logSumExp(t))\n      if (any(is.nan(prob))) prob <- 0\n      if (all(prob == 0)) {\n        print('warning - phi prob NaN or Zero (single)')\n      }\n    }\n    else {\n      t <- rowSums(discMat[, obs.indices])\n      prob <- exp(t - logSumExp(t))\n      if (any(is.nan(prob))) prob <- 0\n      if (all(prob == 0)) {\n        print(obs.indices)\n      }\n    }\n\n        if (all(prob == 0))\n      state$phi[obs.indices] <- NA\n    else\n      state$phi[obs.indices] <- sample(LCACHED$phi, 1, replace = F, prob = prob)\n  }\n\n    state\n}\n\n\n# mean precision\nlog.beta.binomial <- function(b, d, m, s) {\n  # alpha = sm\n  # beta = s(1-m)\n  # (d b) B(b + sm, d - b + s(1-m)) / B(sm, s(1-m))\n  r <- dbetabinom.ab(x=b, size=d, shape1=s*m, shape2=s*(1-m), log=T)\n  r\n}\n\nbeta.binomial <- function(b, d, m, s) {\n  # alpha = sm\n  # beta = s(1-m)\n  # (d b) B(b + sm, d - b + s(1-m)) / B(sm, s(1-m))\n  dbetabinom.ab(x=b, size=d, shape1=s*m, shape2=s*(1-m), log=F)\n}\n\n\n# --- Other ---\n\npyclone.gamma.params <- function(data, proposalS) {\n  b = data * proposalS\n  a = b * data\n\n  list(a=a, b=b)\n}\n\n\nsample.gamma.proposal <- function(mean, proposalS=.01) {\n  params <- pyclone.gamma.params(mean, proposalS)\n  # shape and rate have to be positive\n  rgamma(n=1, params$a, params$b)\n}\n\n\n\nr.beta.binomail <- function(d, m, s) {\n  rbetabinom.ab(1, size=d, shape1=s*m, shape2=s*(1-m))\n}\n\n# example ddcrp summary functions\nncomp.summary <- function(dat, iter, state, lhood, alpha)\n{\n  c(iter = iter, ncomp = length(unique(state$cluster)))\n}\n\ncrp.score.summary <- function(dat, iter, state, lhood, alpha)\n{\n  c(iter = iter,\n    ncomp = length(unique(state$cluster)),\n    crp.score = (crp.log.prior(alpha, state$cluster) + sum(lhood)))\n}\n\n\n# returns customers that are connected to i, directly or indirectly, inclusing i\n# NOT THOSE THAT i is connected to!!! (3->i<-4, i->5) returns c(3,4) and NOT 5\nconnections <- function(i, links)\n{\n  visited <- c()\n  to.visit <- c(i)\n  while (length(to.visit) > 0) {\n    curr <- to.visit[1]\n    visited <- c(visited, curr)\n    to.visit <- to.visit[-1]\n    pointers <- which(links == curr)\n    for (p in pointers) {\n      if (!(p %in% visited))\n        to.visit <- c(to.visit, p)\n    }\n  }\n  visited\n}\n\nsample.processor <- function(state, hyperParams, iter) {\n  state <- cached.sample.pyclone.cluster.parameters(state, hyperParams$s)\n  fileName <- paste0(iter, '-clust.csv')\n  write.table(state[, c(2, 6)], file.path(expDir, fileName), sep='\\t', col.names=F, row.names=F, append = T)\n}\n\n",
    "created" : 1467240781163.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2525887901",
    "id" : "45938A25",
    "lastKnownWriteTime" : 1467252331,
    "path" : "~/Google Drive/Masters/Thesis/scripts/ddcrppaper/submission.code/ddclone/R/helper.R",
    "project_path" : "R/helper.R",
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "type" : "r_source"
}